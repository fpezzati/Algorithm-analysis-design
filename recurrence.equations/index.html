<!DOCTYPE html>
<html>
<head>
<title>Recurrence equations</title>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
</script>
<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
</head>
<body>
  Recurrence equations are model who implements something evolving while they work
  on a set of input. They are an invaluable tool to analyze costs about algorithms
  who use recursive call and can be modeled by recurrence equations.
  Recurrence equation is a recursive formula who express a sequence's n-th element
  in relation with the previous $n-1$ elements. Recurrence equation is of order
  $r$ when n-th element is directly related with $n-1, n-2, \dots, n-r$ previous elements.
  Recurrence equations are often used to express recursive algorithms behavior.
  Just think about mergesort complexity. We can describe how it behave by the
  recurrence equation:
  $$ T(n) = T(n/2) + T(n/2) + n, C_0 = 1;  $$
  There are a few recurrence equation types:
  <ul>
    <li>
      <h4>first order</h4>
      When $T(n)$ is bound to $T(n-1)$. Example:
      $$
      \begin{cases}
      T(n) = T(n-1) + \Theta(1) \\
      T(1) = \Theta(1)
      \end{cases}
      $$
      <ul>
        <li>
          <h4>linear</h4>
          When $T(n)$ is a polynomial of degree one and it is in the form of
          $t_nx_n+t_{n-1}x_{n-1} \cdots t_{n+k}x_{n+k}+t_0 = 0$.
          <ul>
            <li>with constant coefficients</li>
            <li>with not constant coefficients</li>
          </ul>
        </li>
        <li>
          <h4>not linear</h4>
          When $T(n)$ is a polynomial of degree greater than one.
        </li>
      </ul>
    </li>
    <li>
      <h4>second order</h4>
      When $C_n$ is bound to $C_{n-1}$ and $C_{n-2}$. Let's say:
      $$
      \begin{cases}
      T(n) = T(n-1) + T(n-2) + \Theta(1) \\
      T(1) = \Theta(1)
      \end{cases}
      $$
      <ul>
        <li>linear</li>
        <li>not linear</li>
        <li>with variables</li>
      </ul>
    </li>
    <li>order $t$</li>
    <li>full history</li>
    <li>divide et impera</li>
  </ul>
  Recurrence equations are represented by a "first step" equation and the recursive
  part.
  $$
  \begin{cases}
  T(n) = T(n-1)+\Theta(1) \\
  T(1) = \Theta(1)
  \end{cases}
  $$
  The recursive part should be always made by the recursive segment ($T(n-1)$) and
  a constant who represent the effort the algorithm required to compute all that
  is not part of recursive step. The non-recursive step part can't be ignored because
  it is always present in algorithms and it states that input set size matters.
  Problems arise when you want to translate recurrence equantions in a equation
  you can use to analyze algorithm's asymptotic behavior. So, how to deal with recurrence
  equations? There are few ways.
  <ul>
    <li>
      <h4>Substitution approach</h4>
      You guess a solution and check if it works. You have to guess it as near as
      possible to solution. All the guess smaller than solution will work if we
      are tryin to find a $\Omega$, and all the guess larger than solution will work
      if we are tryin to find a $O$.
    </li>
    <li>
      <h4>Iterative approach</h4>
      You iterate $T(n)$ enought times to obtain a sum of terms depending by $n$
      and by initial conditions. It could be used to solve divide et impera algorithms.
    </li>
    <li>
      <h4>Tree approach</h4>
      Let $T(n)$ be the root of our tree. You can determine it's children by appying
      the first step of Iterative approach, then you can apply again that first
      step to children's children and so on until you reach tree's leafs $(T(1))$.
      This approach looks to me like a graphical representation of the Iterative
      approach (personal opinion, you are free to disagree).
    </li>
    <li>
      <h4>Master method (or Repertorio approach)</h4>
      It is a useful way to solve recurrence equations which are in the form $T(n) = aT(\frac{n}{b}) + f(n)$.
      Where $a \geq 1$ indicates that problem splits in at least $a$ subproblems
      at the iterative step. $b \gt 1$ indicates how much problem's size is downsized
      at the iterative step. $f(n) = \Theta(n^c), c \geq 0$ implements time to split
      original problem.
    </li>
    <li>
    </li>
  </ul>
  <h4>Substitution approach</h4>
  Let be
  $$
  \begin{cases}
  T(n) = T(n-1)+\Theta(1) \\
  T(1) = \Theta(1)
  \end{cases}
  $$
  the algorithm we want to shape in a linear form to study it's asymptotic behavior.
  We move Landau notation about costants:
  $$
  \begin{cases}
  T(n) = T(n-1)+c \\
  T(1) = d
  \end{cases}
  $$
  and we guess that this algorithm has $T(n) = O(n)$ behavior, so $T(n) \leq kn$.
  So $T(1) = k$ and this is fine if $k \leq d$. Now
  $$
  \begin{align}
  T(n) \leq k(n-1)+c = kn-k+c \leq kn \\
  -k \leq -c \\
  k \geq c
  \end{align}
  $$
  So solution is solid if $k \geq c$ and $k \geq d$. A $k$ value like can be always
  found so solution is solid: algorithm has a $O(n)$ behavior.
  $$
  \begin{cases}
  T(n) = T(n-1)+\Theta(1) \\
  T(1) = \Theta(1)
  \end{cases}
  $$
  <h4>Iterative approach</h4>
  In the iterative approach you find the linear equation by exploding the $n$th
  iterative step in the $n-1$th plus $\Theta(n)$. Let's put this simple. Let
  $$
  \begin{cases}
  T(n) = T(n-1)+\Theta(1) \\
  T(1) = \Theta(1)
  \end{cases}
  $$
  be the functions who describe our algorithm. The algorithm is the sequential search.
  Explode $T(n)$ in $T(n-1)+\Theta(1)$ and $T(n-1)$ in $T(n-2)+\Theta(1)$ and so
  on:
  $$
  T(n) = T(n-1) + \Theta(1) = T(n-2)+\Theta(1)+\Theta(1) = \ldots = n \Theta(1) = n
  $$
  this is a simple case. Let's try iterative approach agains a binary search algorithm.
  Binary search can be represented as:
  $$
  \begin{cases}
  T(n) = T(n/2)+\Theta(1) \\
  T(1) = T(0) = \Theta(1)
  \end{cases}
  $$
  Let's apply the iterative approach:
  $$
  T(n) = T(n/2) + \Theta(1) = T(n/2^2) + \Theta(1) + \Theta(1) = \\
  T(n/2^3) + \Theta(1) + \Theta(1) + \Theta(1) = \ldots = \\
  T(n/2^n) + n \Theta(1) = \log_2 n \Theta(n) = \Theta(log_2 n)
  $$
  Now let's try the iterative approach against algorithm who compute Fibonacci's
  series.
  $$
  \begin{cases}
  T(n) = T(n-1) + T(n-2) + \Theta(1) \\
  T(1) = T(0) = 1
  \end{cases}
  $$
  If we try to substitute equations with their previous steps we will see that operands
  grows in number too fast and we can't find a linear representation. But we can
  say that $T(n) \leq 2T(n-1)+\Theta(1)$ and $T(n) \geq 2T(n-2)+\Theta(1)$. We can
  use these two observations to find linear representations of $O$ and $\Theta$
  about the algorithm. Let's start with $O$:
  $$
  T(n) \leq 2T(n-1)+\Theta(1) \leq 2^2T(n-2)+2^1\Theta(1)+\Theta(1) \leq \\
  2^3T(n-3)+2^2\Theta(1)+2^1\Theta(1)+\Theta(1) \leq \\
  \ldots \\
  \leq 2^kT(n-k)+\sum_{i=0}^{k-1}2^i\Theta(1) \leq 2^{n-1}\Theta(1)+\sum_{i=0}^{n-2}2^i\Theta(1) = \\
  2^{n-1}\Theta(1)+(2{n-1}-1)\Theta(1) = (2^n-1)\Theta(1) \Rightarrow \\
  T(n) = O(2^n)
  $$
  $T(n)$'s behavior finds it's superior limit in $O(2^n)$. Now let's find $T(n)$'s behavior
  limit:
  $$
  T(n) \geq 2T(n-2)+\Theta(1) \geq 2^2T(n-2\cdot2)+2\Theta(1) \geq 2^3T(n-2\cdot3)+2^2\Theta(1)+2^1\Theta(1)+2^0\Theta(1) \geq \\
  \ldots \\
  \geq 2^kT(n-2k)+\sum_{i=0}^{k-1}\Theta(1) \geq 2^{\frac{n}{2}}\Theta(1)+\sum_{i=0}^{\frac{n}{2}-1}2^i\Theta(1) = \\
  2^{\frac{n}{2}}\Theta(1) + (2^{\frac{n}{2}}-1)\Theta(1) = (2\cdot2^{\frac{n}{2}}-1)\Theta(1) \Rightarrow \\
  T(n) = \Omega(2^{\frac{n}{2}})
  $$
  So we can say that T(n) asymptotical behavior's inferior limit is $\Omega(2^{\frac{n}{2}})$.
  <h4>Tree approach</h4>
  Tree approach is a variant of the iterative approach.
  <h4>Master method (or Repertorio approach)</h4>
  We say Master method solves recurrences who:
  <ul>
    <li>
      are in the form of $T(n) = a(T \frac{n}{b})+f(n)$,
    </li>
    <li>
      $a \geq 1$. The problem is at least reduced to a smaller sub problem by
      the recursive step and at least one step is required,
    </li>
    <li>
      $b \geq 1$. Recursive step splits the problem in smaller one. Size of subproblem
      is reduced by a factor $b$,
    </li>
    <li>
      $f(n) \geq 0$ generally.
    </li>
  </ul>
  So, given the initial problem each recursive step splits that one in $a$ smaller
  ones each one reduced in size by a $b$ factor. Exploding problems in sub problems
  for each steps can be seen as a tree of height $H = log_b n$ who has
  $$a^H = a^{\log_b n} \Rightarrow n^{\log_b a}$$
  leaves.
  Let's consider these three cases:
  <ul>
    <li>
      Let the number of leaves grow faster than $f(n)$. In this case cost of each
      step increase by a significant factor and $f(n)$ will be significant smaller
      the cost to reach that leaf at the time we reach it. This means that step's
      cost dominates. Then we can say:
      $$T(n) = \Theta(n^{log_b a})$$
    </li>
    <li>
      Let the cost of each step be constant. In this case $f(n)$ should be roughly
      equals to $n^{\log_b a}$. So, total running time would be $f(n)$ times the
      tree's height. Then we can say:
      $$T(n) = \Theta(n^{\log_b a} \cdot \log_{k+1} n)$$
      Where $\log_{k+1}$ would be the height of the tree.
    </li>
    <li>
      Let the cost of each step be significantly reduced every time and by the time
      we reach the leaf $f(n)$ will be polynomially larger than $n^{\log_b a}$.
      Then we can say:
      $$T(n) = \Theta(f(n))$$
    </li>
  </ul>
  So, if you have a recursive equation in the form of $T(n) = a(T \frac{n}{b})+f(n)$
  you can solve it by one of the three cases. When cost of recursive steps dominates,
  $$T(n) = \Theta(n^{\log_b a})$$
  When recursive step's cost are equals to leaf cost then:
  $$T(n) = \Theta(n^{\log_b a} \cdot \log_{k+1} n)$$
  When cost to compute single leaves dominates then:
  $$T(n) = \Theta(f(n))$$
</body>
</html>
