<!DOCTYPE html>
<html>
<head>
<title>Algorithm, analysis and design</title>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
</script>
<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
</head>
<body>
  <h3>Algorithm, analysis and design</h3>
  To study algorithms behavior and to describe them by a mathematical point of view.
  Describing algorithm by math give us a exact criteria to compare performaces.
  Criterias are implemented by a mathematical model (a function) and are used to
  see how an algorithm behave in the optimal case, in the average case and in it's
  worst case. So an algorithm may have more than one model to describes how it behave.
  <h4>Landau notation</h4>
  Landau notation comes to help classify algorithm behavior. Given some functions
  $f:\mathbb{N} \to \mathbb{R}$. We can say $f(n)$ is in $O(g(n))$ if
  $$ \lim_{n \to \infty} \frac{f(n)}{g(n)} \lt \infty$$
  $f(n)$ is in the same order of $g(n)$ means that exists constant $K$ that:
  $$  \lim_{n \to \infty} \frac{f(n)}{Kg(n)} \lt 1 $$
  $f(n)$ is in an order inferior than $g(n)$ if
  $$  \lim_{n \to \infty} \frac{f(n)}{g(n)} \lt 0 $$
  Let say $n$ is the data set size our algorithm will work on. So when we say that
  an algorithm is in $O(n ln n)$ order we say that we can represent the algorithm
  behavior by function $f(n)$ and generally speaking $f(n)$ behaves like $n ln n$,
  performances are similar or differ by a constant factor, call it $K$.

  <h3>$ O $ function</h3>
  $$ \lim_{n \to \infty} \frac{f(n)}{g(n)} \lt \infty$$
  $O(f(n))$ indicates all the $g(n)$ function that $|f(n)/g(n)|$ has a superior
  limit by $n \to \infty$. We use $O$ to indicate algorithm's behavior in the worst
  case. So $O$ indicates algorithm's implicit complexity.

  <h3>$ \Omega $ function</h3>
  $$ \lim_{n \to \infty} \frac{g(n)}{Kf(n)} \lt \infty$$
  $\Omega(f(n))$ indicates all the $g(n)$ function that $|g(n)/f(n)|$ has an inferior
  limit in $K$ strictly positive ($\lim_{n \to \infty} \frac{g(n)}{f(n)} \lt K$). We use $\Omega$ to indicate
  algorithm's behavior in the average case.

  <h3>$ \Theta $ function</h3>
  $$ \lim_{n \to \infty} \frac{g(n)}{f(n)} \lt \infty$$
  $$ \lim_{n \to \infty} \frac{f(n)}{g(n)} \lt \infty$$
  $\Theta(f(n))$ indicates all the $g(n)$ function that $|g(n)/f(n)|$ has both superior
  and inferior limit by $n \to \infty$. We use $\Theta$ notation to indicate algorithm's
  behavior in the optimal case (really? Investigate).

  <h3>Computational complexity</h3>
  Study algorithm's performance by observing the increase of computational enforce
  in the worst case. Finding algorithm's enforce superior limit permit to compare
  algorithms performances in the worst case.

  <h3>Average effort</h3>
  Let $\sqcap_n$ be the set of all the input of size $n$ about an algorithm and
  let $\sqcap_{n,k}$ be the set of all the input of size $n$ the algorithm can elaborate
  at cost of $k$. Then
  $$\mu = \frac{\sum_k k \sqcap_{n,k}}{\sqcap_n}$$
  is average cost of the algorithm.

  <h3>Variance</h3>
  Let $\sqcap_n$ be the set of all the input of size $n$ elaborated by an algorithm.
  Then:
  $$\sigma^2 = \frac{\sum_k(k-\mu)^2\sqcap_{n,k}}{\sqcap_n} = \frac{\sum_kk^2\sqcap_{n,k}}{\sqcap_n} - \mu^2$$
  $\sigma^2$ indicates variance and $\sigma$ indicates standard deviation. In other
  words: let $X \in {X_1 \ldot X_n}$ be a random variable and let $m = M(X)$ $X$'s
  average value. $m$ will represent a good information if $|m - X_k|$ is small (with
  $k \in {1 \ldot n} of course). Variance represent sum of differences between $m$
  and $X$ powered by 2:
  $$\sigma^2 = M(X-m)^2 = \sum_{k=1}^{n}(M(X)-X_k)p_k$$
  where $p_k$ is chance about $X_k$ to occurr.

  <h3> Chebyshev's inequality</h3>
  Let $X$ be a random variable with mean (read expected value) $E(X) = \mu$ and
  variance $\sigma^2 = var(X)$. Then Chebyshev's inequality states that:
  $$P(|X - \mu| \gte t) \lte \frac{\sigma^2}{t^2}$$
  for any $t \gt 0$. So the probability that $X$ differ from it's expected value
  $\mu$ by a constant $t$ value is equal to variance divided by $t^2$.
  To prove Chebyshev's inequality we will use Markov's inequality. Markov's inequality
  states that:
  $$P(X \gte t) \lte \frac{E(X)}{t}$$
  We can replace $X$ with $|X-\mu|$, it is still a random variable and Markov's
  inequality still holds,
  $$P(|X - \mu |\gte t) \lte \frac{E(|X - \mu|)}{t}$$
  We can observe that if $P(|X - \mu |\gte t)$ is true, then $P((X-\mu)^2 \gte t^2)$
  is also true. Inequality holds for any value of $X$ and $\mu$. So:
  $$P((X-\mu)^2 \gte t^2) \lte \frac{E((X - \mu)^2)}{t^2}$$
  but $E((X - \mu)^2) = \sigma^2$ it is variance definition in fact.
  Why Chebyshev's inequality is important?
  Let suppose we know expected value about $X$, $E(X) = \mu$ and variance $Var(X) = \sigma^2$.
  Inequality states that $P(|X - \mu| \gte t) \lte \frac{E(|X - \mu|)}{t}$, in other
  words, probability than $X$ is far as $t$ from $\mu$ is less than $\frac{1}{t^2}$.
  Let ${X_1 \ldot X_n}$ all the effort about an algorithm. We expect than all algorithm's
  effort will be near to $|X - \mu|$ as values and efforts can be far from $|X-\mu|$
  with probability of $\frac{1}{t^2}$. Chebyshev's inequality gives us a limit about
  algorithms performance.
</body>
</html>
