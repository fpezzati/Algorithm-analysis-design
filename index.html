<!DOCTYPE html>
<html>
<head>
<title>Algorithm, analysis and design</title>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
</script>
<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
</head>
<body>
  <h3>Algorithm, analysis and design</h3>
  To study algorithms behavior and to describe them by a mathematical point of view.
  Describing algorithm by math give us a exact criteria to compare performaces.
  Criterias are implemented by a mathematical model (a function) and are used to
  see how an algorithm behave in the optimal case, in the average case and in it's
  worst case. So an algorithm may have more than one model to describes how it behave.
  <h4>Landau notation</h4>
  Landau notation comes to help classify algorithm behavior. Given some functions
  $f:\mathbb{N} \to \mathbb{R}$. We can say $f(n)$ is in $O(g(n))$ if
  $$ \lim_{n \to \infty} \frac{f(n)}{g(n)} \lt \infty$$
  $f(n)$ is in the same order of $g(n)$ means that exists constant $K$ that:
  $$  \lim_{n \to \infty} \frac{f(n)}{Kg(n)} \lt 1 $$
  $f(n)$ is in an order inferior than $g(n)$ if
  $$  \lim_{n \to \infty} \frac{f(n)}{g(n)} \lt 0 $$
  Let say $n$ is the data set size our algorithm will work on. So when we say that
  an algorithm is in $O(n ln n)$ order we say that we can represent the algorithm
  behavior by function $f(n)$ and generally speaking $f(n)$ behaves like $n ln n$,
  performances are similar or differ by a constant factor, call it $K$.

  <h3>$ O $ function</h3>
  $$ \lim_{n \to \infty} \frac{f(n)}{g(n)} \lt \infty$$
  $O(f(n))$ indicates all the $g(n)$ function that $|f(n)/g(n)|$ has a superior
  limit by $n \to \infty$. We use $O$ to indicate algorithm's behavior in the worst
  case. So $O$ indicates algorithm's implicit complexity.

  <h3>$ \Omega $ function</h3>
  $$ \lim_{n \to \infty} \frac{g(n)}{Kf(n)} \lt \infty$$
  $\Omega(f(n))$ indicates all the $g(n)$ function that $|g(n)/f(n)|$ has an inferior
  limit in $K$ strictly positive ($\lim_{n \to \infty} \frac{g(n)}{f(n)} \lt K$). We use $\Omega$ to indicate
  algorithm's behavior in the average case.

  <h3>$ \Theta $ function</h3>
  $$ \lim_{n \to \infty} \frac{g(n)}{f(n)} \lt \infty$$
  $$ \lim_{n \to \infty} \frac{f(n)}{g(n)} \lt \infty$$
  $\Theta(f(n))$ indicates all the $g(n)$ function that $|g(n)/f(n)|$ has both superior
  and inferior limit by $n \to \infty$. We use $\Theta$ notation to indicate algorithm's
  behavior in the optimal case (really? Investigate).

  <h3>Computational complexity</h3>
  Study algorithm's performance by observing the increase of computational enforce
  in the worst case. Finding algorithm's enforce superior limit permit to compare
  algorithms performances in the worst case.

  <h3>Average effort</h3>
  Let $\sqcap_n$ be the set of all the input of size $n$ about an algorithm and
  let $\sqcap_{n,k}$ be the set of all the input of size $n$ the algorithm can elaborate
  at cost of $k$. Then
  $$\mu = \frac{\sum_k k \sqcap_{n,k}}{\sqcap_n}$$
  is average cost of the algorithm.

  <h3>Variance</h3>
  Let $\sqcap_n$ be the set of all the input of size $n$ elaborated by an algorithm.
  Then:
  $$\sigma^2 = \frac{\sum_k(k-\mu)^2\sqcap_{n,k}}{\sqcap_n} = \frac{\sum_kk^2\sqcap_{n,k}}{\sqcap_n} - \mu^2$$
  $\sigma^2$ indicates variance and $\sigma$ indicates standard deviation. In other
  words: let $X \in {X_1 \ldots X_n}$ be a random variable and let $m = M(X)$ $X$'s
  average value. $m$ will represent a good information if $|m - X_k|$ is small (with
  $k \in {1 \ldots n}$ of course). Variance represent sum of differences between $m$
  and $X$ powered by 2:
  $$\sigma^2 = M(X-m)^2 = \sum_{k=1}^{n}(M(X)-X_k)p_k$$
  where $p_k$ is chance about $X_k$ to occurr.

  <h3> Chebyshev's inequality</h3>
  Let $X$ be a random variable with mean (read expected value) $E(X) = \mu$ and
  variance $\sigma^2 = var(X)$. Then Chebyshev's inequality states that:
  $$P(|X - \mu| \ge t) \le \frac{\sigma^2}{t^2}$$
  for any $t \gt 0$. So the probability that $X$ differ from it's expected value
  $\mu$ by a constant $t$ value is equal to variance divided by $t^2$.
  To prove Chebyshev's inequality we will use Markov's inequality. Markov's inequality
  states that:
  $$P(X \ge t) \le \frac{E(X)}{t}$$
  We can replace $X$ with $|X-\mu|$, it is still a random variable and Markov's
  inequality still holds,
  $$P(|X - \mu |\ge t) \le \frac{E(|X - \mu|)}{t}$$
  We can observe that if $P(|X - \mu |\ge t)$ is true, then $P((X-\mu)^2 \ge t^2)$
  is also true. Inequality holds for any value of $X$ and $\mu$. So:
  $$P((X-\mu)^2 \ge t^2) \le \frac{E((X - \mu)^2)}{t^2}$$
  but $E((X - \mu)^2) = \sigma^2$ it is variance definition in fact.
  Why Chebyshev's inequality is important?
  Let suppose we know expected value about $X$, $E(X) = \mu$ and variance $Var(X) = \sigma^2$.
  Inequality states that $P(|X - \mu| \ge t) \le \frac{E(|X - \mu|)}{t}$, in other
  words, probability than $X$ is far as $t$ from $\mu$ is less than $\frac{1}{t^2}$.
  Let ${X_1 \ldots X_n}$ all the effort about an algorithm. We expect than all algorithm's
  effort will be near to $|X - \mu|$ as values and efforts can be far from $|X-\mu|$
  with probability of $\frac{1}{t^2}$. Chebyshev's inequality gives us a way to
  analyze algorithms performances.

  <h3>Combinations permutations and dispositions</h3>
  Given a set $N = {0, 1, \dots, n}$.
  Combinations is a selection of some members
  of $N$ where order of selection's element does not matter. ${1, 5, k}$ is good
  as ${k, 1, 5}$. Let $N_3$ be the set of all selection of three elements of $N$,
  ${1, 5, k}$ and ${k, 1, 5}$ are the same element in $N_3$. Combination does not
  admit repetition. A $k$-combination of a set $N$ is a subset of $k$ distinct elements
  of $N$. Let say $N$ has $n$ elements, the number of $k$-combinations of $N$ elements
  are:
  $$\binom{n}{k} = \frac{n(n-1)\dots(n-k+1)}{k(k-1)\dots1} = \frac{n!}{k!(n-k)!},$$
  the binomial coefficient.

  Permutation is a selection of some members of $N$ where selection's element order
  matter. Let $N_3$ the set of all selection of three elements of $N$, then ${1, 5, k}$
  and ${k, 1, 5}$ are two distinct elements in $N_3$.
  Permutations are simple or with repetitions.
  Simple permutations does not admit repetitions, all the element of the set are
  distinct. They are easy to compute. Let say $|N|=n$, all the possible $N$'s elements
  permutations are:
  $$P_n = n!.$$
  Permutation with repetitions admit sets where two or more elements are equals.
  Let say $|N| = n$ and $\alpha \lt n$ are $N$'s equals elements. Then:
  $$P_{n}^{\alpha} = \frac{n!}{\alpha!},$$
  indicates all the possible distinct permutations of $N$'s elements. Let say $|N|=n$
  and $\alpha = 3$ and $\beta = 2$. Then all the possible distinct permutations
  with $|\alpha|$ and $|\beta|$ identical elements are:
  $$P_{n}^{\alpha, \beta} = \frac{n!}{\alpha! \beta!}.$$

  Disposition indicates all the posible permutations of $k$ elements in a $|N|=n$
  set, where $n \gt k$. Dispositions are simple or with repetitions.
  Simple disposition expects that all the elements of the given set, say $N$, are
  distinct. So, given $|N|=n$, all the simple dispositions of $k$ elements are:
  $$D_{(n,k)} = \frac{n!}{(n-k)!}.$$
  Disposition with repetitions expects that:
  <ul>
    <li>you pick $k$ elements from $N$ for each, disposition</li>
    <li>the same element can occur up to $k$ times in a disposition</li>
    <li>dispositions differ from each other by at least one element or elements order</li>
  </ul>
</body>
</html>
